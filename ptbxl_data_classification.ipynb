{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas and code partly taken from https://www.kaggle.com/code/jraska1/ptb-xl-ecg-1d-convolution-neural-network - most of it - EMPTY DATA FILLING WHICH IS NOT ESSENCTIAL FOR THE ECG SIGNAL CLASSIFICATION.\n",
    "\n",
    "The purpose of this notebook is to create a prediction model, which takes into account the metadata about patient and the samples as the ECG curves. Targets for the model will be superclasses as defined by the dataset.\n",
    "\n",
    "Superclasses enumerated by dataset description are as follows:\n",
    "```\n",
    "Records | Superclass | Description\n",
    "9528 | NORM | Normal ECG\n",
    "5486 | MI | Myocardial Infarction\n",
    "5250 | STTC | ST/T Change\n",
    "4907 | CD | Conduction Disturbance\n",
    "2655 | HYP | Hypertrophy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import wfdb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all I need to load metadata about patients and samples provided by dataset. All metadata will be loaded to **ECG_df** and **SCP_df** dataframes respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = r'D:\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/'\n",
    "\n",
    "ECG_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'ptbxl_database.csv'), index_col='ecg_id')\n",
    "ECG_df.scp_codes = ECG_df.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "ECG_df.patient_id = ECG_df.patient_id.astype(int)\n",
    "ECG_df.nurse = ECG_df.nurse.astype('Int64')\n",
    "ECG_df.site = ECG_df.site.astype('Int64')\n",
    "ECG_df.validated_by = ECG_df.validated_by.astype('Int64')\n",
    "\n",
    "SCP_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'scp_statements.csv'), index_col=0)\n",
    "SCP_df = SCP_df[SCP_df.diagnostic == 1]\n",
    "\n",
    "ECG_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-03T11:17:42.134369Z",
     "iopub.status.busy": "2022-11-03T11:17:42.134005Z",
     "iopub.status.idle": "2022-11-03T11:17:42.142098Z",
     "shell.execute_reply": "2022-11-03T11:17:42.140458Z",
     "shell.execute_reply.started": "2022-11-03T11:17:42.134338Z"
    }
   },
   "source": [
    "ECG samples are strattified to 10 groups. The authors of PTB-XL ECG dataset suggest use first 8 groups as the training samples. Last two groups then use as the validation and test sample set. \n",
    "I will accept this suggestion on my following work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECG_df.strat_fold.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to add one more column **scp_classes** to ECG_df dataset, which represents all superlasses (as a list of abbreviations) assigned to the sample by cardiologists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_class(scp):\n",
    "    res = set()\n",
    "    for k in scp.keys():\n",
    "        if k in SCP_df.index:\n",
    "            res.add(SCP_df.loc[k].diagnostic_class)\n",
    "    return list(res)\n",
    "                    \n",
    "ECG_df['scp_classes'] = ECG_df.scp_codes.apply(diagnostic_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First problem, I would like to cope with, are null values in metadata dataframe. There is a quick look at the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "msno.matrix(ECG_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to add another angle of the view, there is an overview of unique values in all columns of metadata dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECG_df[[col for col in ECG_df.columns if col not in ('scp_codes', 'scp_classes')]].nunique(dropna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for modeling\n",
    "\n",
    "I need first prepare input and output (targets) for my models. \n",
    "\n",
    "As inputs I will use both patient metadata (now loaded in the ECG_df dataframe) and ECG curves (in the ECG_data numpy array) respectively. But both require some rework to be useful for modeling, which will be done in following few steps.\n",
    "\n",
    "As outputs I will create new dataframe with rows equal to samples and columns corresponding with diagnosis superclasses.\n",
    "\n",
    "Because I will have two inputs and one output, I will preffix all created dataframes as follows:\n",
    "- X - prefix for patient and sample metadata\n",
    "- Y - prefix for ECG curves\n",
    "- Z - prefix for targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X dataframe ...\n",
    "\n",
    "I won't use all columns from ECG_df dataframe, but only a subset of them. \n",
    "Created dataframe **X** comprises only columns, witch are related to patient, his health condition and the path of ECG and device with which ECG was recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(index=ECG_df.index)\n",
    "\n",
    "X['patient_id'] = ECG_df.patient_id\n",
    "\n",
    "X['age'] = ECG_df.age\n",
    "X.age.fillna(0)\n",
    "\n",
    "X['sex'] = ECG_df.sex.astype(float)\n",
    "X.sex.fillna(0)\n",
    "\n",
    "X['height'] = ECG_df.height\n",
    "X.loc[X.height < 50, 'height'] = np.nan\n",
    "X.height.fillna(0)\n",
    "\n",
    "X['device'] = ECG_df.device\n",
    "\n",
    "X['weight'] = ECG_df.weight\n",
    "X.weight.fillna(0)\n",
    "\n",
    "X['infarction_stadium1'] = ECG_df.infarction_stadium1.astype(str).replace({\n",
    "    'unknown': 0,\n",
    "    'Stadium I': 1,\n",
    "    'Stadium I-II': 2,\n",
    "    'Stadium II': 3,\n",
    "    'Stadium II-III': 4,\n",
    "    'Stadium III': 5\n",
    "}).fillna(0)\n",
    "\n",
    "X['infarction_stadium2'] = ECG_df.infarction_stadium2.astype(str).replace({\n",
    "    'unknown': 0,\n",
    "    'Stadium I': 1,\n",
    "    'Stadium II': 2,\n",
    "    'Stadium III': 3\n",
    "}).fillna(0)\n",
    "\n",
    "X['pacemaker'] = (ECG_df.pacemaker == 'ja, pacemaker').astype(float)\n",
    "\n",
    "X['filename_lr'] = ECG_df.filename_lr\n",
    "X['filename_hr'] = ECG_df.filename_hr\n",
    "\n",
    "X['total_count'] = X.groupby('patient_id')['patient_id'].transform('count')\n",
    "\n",
    "# Count unique items\n",
    "unique_count = X['patient_id'].nunique()\n",
    "print(f'Unique patiens {unique_count}')\n",
    "\n",
    "X_sort = X.sort_values('total_count')\n",
    "X_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the ending of dataframe more\n",
    "print(X.tail(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X['device'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_unique_device_ECG(df: pd.DataFrame) -> None:\n",
    "    # Count the unique values in the 'device' column\n",
    "    device_counts = df['device'].value_counts()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    \n",
    "    # Plot the bar chart on the given axis\n",
    "    device_counts.plot(kind='bar', ax=ax)\n",
    "    \n",
    "    ax.set_facecolor('white')\n",
    "    ax.spines['top'].set_color('gray')\n",
    "    ax.spines['right'].set_color('gray')\n",
    "    ax.spines['bottom'].set_color('gray')\n",
    "    ax.spines['left'].set_color('gray')\n",
    "    ax.grid(True, color='gray', alpha=0.65, linewidth=0.5, linestyle='dashed')\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title('ECGs Counts with a Different Device')\n",
    "    ax.set_xlabel('Device')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "    # Annotate each bar with its value\n",
    "    for i in range(len(device_counts)):\n",
    "        ax.text(i, device_counts.iloc[i] + 50, str(device_counts.iloc[i]), ha='center')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_unique_device_ECG(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z targets ...\n",
    "\n",
    "I am going to create **Z** dataframe with columns corresponding to diagnoses superclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pd.DataFrame(0, index=ECG_df.index, columns=['NORM', 'MI', 'STTC', 'CD', 'HYP'], dtype='int')\n",
    "for i in Z.index:\n",
    "    for k in ECG_df.loc[i].scp_classes:\n",
    "        Z.loc[i, k] = 1\n",
    "\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for rows (signals) with multiple annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = (Z.iloc[:, 1:] == 1).sum(axis=1) > 1  # Skip the first column (ecg_id)\n",
    "\n",
    "# Get rows with multiple 1s\n",
    "rows_with_multiple_ones = Z[mask]\n",
    "\n",
    "print(f'Total signals with multiple annotations: {len(rows_with_multiple_ones)}')\n",
    "print(rows_with_multiple_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting to train, validate and test datasets\n",
    "\n",
    "As the authors of PTB-XL ECG dataset suggest, I will split all input and output dataset to training, validation and test subsets according *strat_fold* column. Let's make different dataframe of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Z_train = X[ECG_df.strat_fold <= 8],Z[ECG_df.strat_fold <= 8]\n",
    "X_valid, Z_valid = X[ECG_df.strat_fold == 9], Z[ECG_df.strat_fold == 9]\n",
    "X_test, Z_test  = X[ECG_df.strat_fold == 10], Z[ECG_df.strat_fold == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train samples: {len(X_train)}')\n",
    "print(f'Valid samples: {len(X_valid)}')\n",
    "print(f'Test samples: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "def draw_ecg_counts(train_df: pd.DataFrame, valid_df: pd.DataFrame, test_df: pd.DataFrame, \n",
    "                    categories=None, top_text_offset=50, y_max=None) -> None:\n",
    "    \"\"\"\n",
    "    Draws a grouped bar chart of total ECG counts for each class in Train, Validation, and Test sets.\n",
    "\n",
    "    Parameters:\n",
    "    train_df, valid_df, test_df (pd.DataFrame): DataFrames containing the ECG data.\n",
    "    categories (list of str, optional): List of category names to include in the plot.\n",
    "                                        Defaults to ['NORM', 'MI', 'STTC', 'CD', 'HYP'].\n",
    "    top_text_offset (int, optional): Offset for the text annotation above each bar. Default is 50.\n",
    "    y_max (int, optional): Maximum limit for the y-axis. Auto-scales if not provided.\n",
    "    \"\"\"\n",
    "    if categories is None:\n",
    "        categories = ['NORM', 'MI', 'STTC', 'CD', 'HYP']\n",
    "    \n",
    "    # Compute total counts for each set\n",
    "    train_counts = train_df[categories].sum()\n",
    "    valid_counts = valid_df[categories].sum()\n",
    "    test_counts = test_df[categories].sum()\n",
    "\n",
    "    # Compute max value dynamically if y_max is not given\n",
    "    max_value = max(train_counts.max(), valid_counts.max(), test_counts.max())\n",
    "    if y_max is None:\n",
    "        y_max = max_value * 1.1  # Add 10% buffer to max value\n",
    "    \n",
    "    # Define bar width dynamically\n",
    "    num_datasets = 3  # Train, Valid, Test\n",
    "    width = 0.8 / num_datasets  # Adjust width dynamically\n",
    "\n",
    "    # Define positions for grouped bars\n",
    "    x = np.arange(len(categories))\n",
    "\n",
    "    # Use plasma colormap for colors\n",
    "    plasma = cm.plasma\n",
    "    colors_train = plasma(0.1)  # Light purple\n",
    "    colors_valid = plasma(0.5)  # Medium yellow\n",
    "    colors_test = plasma(0.9)   # Dark red\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # Plot bars for Train, Valid, Test with plasma colors and transparency (alpha=0.6)\n",
    "    bars1 = ax.bar(x - width, train_counts, width, label='Train', color=colors_train, edgecolor='black', alpha=0.6)\n",
    "    bars2 = ax.bar(x, valid_counts, width, label='Valid', color=colors_valid, edgecolor='black', alpha=0.8)\n",
    "    bars3 = ax.bar(x + width, test_counts, width, label='Test', color=colors_test, edgecolor='black', alpha=0.8)\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_facecolor('white')\n",
    "    ax.set_title('ECG Count for Each Class (Train, Valid, Test)', fontsize=19, fontweight='bold')\n",
    "    ax.set_xlabel('ECG Class', fontsize=17)\n",
    "    ax.set_ylabel('Total Count', fontsize=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(categories, fontsize=20)\n",
    "    \n",
    "    # Add both vertical and horizontal grid lines\n",
    "    ax.grid(True, which='both', axis='both', linestyle='dashed', alpha=0.6, linewidth=0.7, color='gray')\n",
    "\n",
    "    # Make legend bigger\n",
    "    ax.legend(fontsize=22, loc='upper right', frameon=True)\n",
    "\n",
    "    # Set y-axis limit if specified\n",
    "    if y_max is not None:\n",
    "        ax.set_ylim(0, y_max)\n",
    "\n",
    "    # Set y-tick font size directly (no argument needed)\n",
    "    ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    # Add text annotations on top of bars\n",
    "    def add_labels(bars):\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            offset = top_text_offset if height + top_text_offset < (y_max or float('inf')) else height * 0.05\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, height + offset, f\"{int(height)}\",\n",
    "                    ha='center', va='bottom', fontsize=20, fontweight='bold')\n",
    "\n",
    "    add_labels(bars1)\n",
    "    add_labels(bars2)\n",
    "    add_labels(bars3)\n",
    "\n",
    "    # Improve layout and show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_ecg_counts(Z_train, Z_valid, Z_test, y_max=8200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JUST FOR VERIFICATION OF NORM IN TEST SET DO INSPECTION HOW MANY SAMPLES ARE LABELED AS NORM AND HAS OTHER ANNOTATION, NOTICE THIS IN THE INVESTIFATION (CONFUSION MATRICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check TRUE NORM IN TEST \n",
    "norm_count = Z_test[(Z_test[\"NORM\"] == 1) & \n",
    "           ((Z_test[\"MI\"] == 0) & \n",
    "           (Z_test[\"STTC\"] == 0) & \n",
    "           (Z_test[\"CD\"] == 0) & \n",
    "           (Z_test[\"HYP\"] == 0))].shape[0]\n",
    "\n",
    "print(f'Test samples with NORM count: {norm_count}')\n",
    "\n",
    "norm_with_other_count = Z_test[(Z_test[\"NORM\"] == 1) & \n",
    "                       ((Z_test[\"MI\"] != 0) |\n",
    "                       (Z_test[\"STTC\"] != 0) | \n",
    "                       (Z_test[\"CD\"] != 0) |\n",
    "                       (Z_test[\"HYP\"] != 0))].shape[0]\n",
    "print(f'Test samples with NORM and some other annotation in the same signal count: {norm_with_other_count}')\n",
    "print(Z_test[(Z_test[\"NORM\"] == 1) & \n",
    "                       ((Z_test[\"MI\"] != 0) |\n",
    "                       (Z_test[\"STTC\"] != 0) | \n",
    "                       (Z_test[\"CD\"] != 0) |\n",
    "                       (Z_test[\"HYP\"] != 0))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Y - ECG for further processing and checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import wfdb\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sampling_rate = 100\n",
    "\n",
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        filenames = df.filename_lr\n",
    "    else:\n",
    "        filenames = df.filename_hr\n",
    "\n",
    "    # Add tqdm here to show the progress\n",
    "    data = [wfdb.rdsamp(os.path.join(path, f)) for f in tqdm(filenames, desc=\"Loading Data\", unit=\"file\")]\n",
    "    \n",
    "    # Extracting signals\n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Loading training ECGs...')\n",
    "Y_train = load_raw_data(X_train, sampling_rate, PATH_TO_DATA)\n",
    "print(f'Loaded {len(Y_train)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Loading validation ECGs...')\n",
    "Y_valid = load_raw_data(X_valid, sampling_rate, PATH_TO_DATA)\n",
    "print(f'Loaded {len(Y_valid)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Loading testing ECGs...')\n",
    "Y_test = load_raw_data(X_test, sampling_rate, PATH_TO_DATA)\n",
    "print(f'Loaded {len(Y_test)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also very good work on data wrangling was already done at [PTB XL Dataset Wrangling](https://www.kaggle.com/code/khyeh0719/ptb-xl-dataset-wrangling) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "def plot_ecg(ecg: np.array, trend: np.array = None, title: str = 'ECG signal', y_title='Voltage (mV)', sampling_rate: int = 100):\n",
    "    # Standard lead names for 12-lead ECG\n",
    "    lead_names = ['I', 'II', 'III', 'aVL', 'aVR', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "    \n",
    "    fig, axs = plt.subplots(12, 1, figsize=(14, 15), sharex=True)  # Share x-axis for alignment\n",
    "\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        if trend is not None:\n",
    "            ax.plot(trend[:, i], alpha=0.85, linewidth=1.5, linestyle='dashed', color='coral')\n",
    "            fig.legend(['Trend', 'Signal'], loc='upper right', bbox_to_anchor=(0.92, 1.03), ncol=2, prop=FontProperties(size=16))\n",
    "\n",
    "        ax.plot(ecg[:, i], alpha=0.85, linewidth=0.85, color='indigo')\n",
    "        ax.set_facecolor('white')\n",
    "        ax.spines['top'].set_color('gray')\n",
    "        ax.spines['right'].set_color('gray')\n",
    "        ax.spines['bottom'].set_color('gray')\n",
    "        ax.spines['left'].set_color('gray')\n",
    "\n",
    "        ax.grid(True, color='gray', alpha=0.65, linewidth=0.5, linestyle='dashed')\n",
    "        \n",
    "         # Remove extra ticks and set x-limits\n",
    "        ax.set_xticks(np.arange(0, ecg.shape[0] + 1, ecg.shape[0] / 10))  # Customize the number of ticks\n",
    "        ax.set_xlim([0, ecg.shape[0]])  # Set xlim to prevent offset\n",
    "        #if i != len(ecg[1]) - 1:\n",
    "        #    ax.set_xticklabels([])  # Remove x-axis labels\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "        # Compute min and max\n",
    "        y_min, y_max = np.min(ecg[:, i]), np.max(ecg[:, i])\n",
    "        ax.set_ylim([y_min, y_max])  # Set y-axis limits\n",
    "\n",
    "        # Move voltage labels to the left\n",
    "        #ax.text(-ecg.shape[0] * 0.02, (y_max + y_min) / 2, f\"{y_min:.1f} to {y_max:.1f} mV\", \n",
    "        #        fontsize=12, color='black', ha='center', va='center')\n",
    "\n",
    "        # Move lead annotations to the right\n",
    "        ax.text(ecg.shape[0] * 1.025, (y_max + y_min) / 2, lead_names[i], \n",
    "                fontsize=15, color='black', ha='center', va='center')\n",
    "\n",
    "    # Adjust the main title position (moved up slightly)\n",
    "    plt.suptitle(title, fontsize=20, y=1.02)\n",
    "\n",
    "    # Adjust x and y-axis labels positioning\n",
    "    fig.text(0.5, 0.01, f'Time (in samples)', ha='center', va='center', fontsize=16)  # Moved down\n",
    "    fig.text(0.04, 0.5, y_title, ha='center', va='center', rotation='vertical', fontsize=16)  # Moved left\n",
    "\n",
    "    # Adjust layout to ensure labels are well positioned\n",
    "    plt.subplots_adjust(left=0.08, right=0.90, top=0.99, bottom=0.04)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ecg(Y_train[0], title='Original Train ECG Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ecg(Y_valid[0], title='Original Validation ECG Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ecg(Y_test[0], title='Original Test ECG Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def detrend_signal_polynom(ecg_signal, poly_degree=10):\n",
    "    \"\"\"\n",
    "    Detrend ECG signals using polynomial detrending.\n",
    "    \n",
    "    Args:\n",
    "        signal (ndarray): 2D NumPy array representing ECG signals with shape (signal, lead).\n",
    "        poly_degree (int): Degree of polynomial for detrending.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Detrended ECG signals.\n",
    "    \"\"\"\n",
    "    detrended_signals = np.zeros_like(ecg_signal)\n",
    "    trend_curves = np.zeros_like(ecg_signal)\n",
    "    # preallocate once\n",
    "    trend_curve = np.linspace(0, ecg_signal.shape[0], ecg_signal.shape[0])  # Do no take into consideration time, it is datapoint in general\n",
    "    \n",
    "    # Detrend each lead\n",
    "    for i in range(ecg_signal.shape[1]):\n",
    "        #start_time = time.time()\n",
    "        detrended_signal = np.polyfit(trend_curve, ecg_signal[:, i], poly_degree)\n",
    "        trend = np.polyval(detrended_signal, trend_curve)\n",
    "        #end_time = time.time()\n",
    "        #execution_time = end_time - start_time\n",
    "        #print(f\"Execution Time: {execution_time} seconds\")\n",
    "        trend_curves[:, i] = trend\n",
    "        detrended_signals[:, i] = ecg_signal[:, i] - trend\n",
    "    return detrended_signals, trend_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detrend signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended_train_signal, trend_train_curves = detrend_signal_polynom(Y_train[0])\n",
    "plot_ecg(detrended_train_signal, trend_train_curves, title='Detrended Train ECG Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_ecg(trend_train_curves, title='Train Trend Curves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended_valid_signal, trend_valid_curves = detrend_signal_polynom(Y_valid[0])\n",
    "plot_ecg(detrended_valid_signal, trend_valid_curves, title='Detrended Valid ECG Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_ecg(trend_valid_curves, title='Valid Trend Curves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended_test_signal, trend_test_curves = detrend_signal_polynom(Y_test[0])\n",
    "plot_ecg(detrended_test_signal, trend_test_curves, title='Detrended Test ECG Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_ecg(trend_test_curves, title='Test Trend Curves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets chose polinomial approach to baseline all the ECGs... it looks better and it is fast... (TODO: inspect other approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "# Preallocated arrays for the detranded signals\n",
    "Y_train_detrend = np.zeros_like(Y_train)\n",
    "Y_test_detrend = np.zeros_like(Y_test)\n",
    "Y_valid_detrend = np.zeros_like(Y_valid)\n",
    "\n",
    "for i in tqdm(range(Y_train.shape[0]), desc ='Detrending Training'):\n",
    "    Y_train_detrend[i], _ = detrend_signal_polynom(Y_train[i])\n",
    "\n",
    "for i in tqdm(range(Y_test.shape[0]), desc ='Detrending Testing'):\n",
    "    Y_test_detrend[i], _ = detrend_signal_polynom(Y_test[i])\n",
    "    \n",
    "for i in tqdm(range(Y_valid.shape[0]), desc ='Detrending Valid'):\n",
    "    Y_valid_detrend[i], _ = detrend_signal_polynom(Y_valid[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets scale ECGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler \n",
    "#Y_scaler = StandardScaler()\n",
    "#Y_scaler.fit(Y_train_detrend.reshape(-1, Y_train_detrend.shape[-1]))\n",
    "#print(f'Mean: {Y_scaler.mean_}')\n",
    "#print(f'Variance: {Y_scaler.var_}')\n",
    "#print(f'Scale: {Y_scaler.scale_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and reshape back\n",
    "#Y_train_scaled = Y_scaler.transform(Y_train_detrend.reshape(-1, Y_train_detrend.shape[-1])).reshape(Y_train_detrend.shape)\n",
    "#Y_valid_scaled = Y_scaler.transform(Y_valid_detrend.reshape(-1, Y_valid_detrend.shape[-1])).reshape(Y_valid_detrend.shape)\n",
    "#Y_test_scaled  = Y_scaler.transform(Y_test_detrend.reshape(-1, Y_test_detrend.shape[-1])).reshape(Y_test_detrend.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_detrend.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_normalize_sample_lead_wise(ecg_data):\n",
    "    \"\"\"\n",
    "    Apply Z-normalization sample-wise and lead-wise to an ECG signal array.\n",
    "\n",
    "    Parameters:\n",
    "        ecg_data (numpy.ndarray): Input ECG data array of shape (num_samples, num_timesteps, num_leads).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Z-normalized ECG data array of the same shape as input.\n",
    "    \"\"\"\n",
    "    # Initialize an array to store the normalized data\n",
    "    normalized_ecg_data = np.zeros_like(ecg_data)\n",
    "\n",
    "    # Loop through each sample\n",
    "    for sample_idx in range(ecg_data.shape[0]):\n",
    "        # Loop through each lead (signal)\n",
    "        for lead_idx in range(ecg_data.shape[2]):\n",
    "            # Extract the lead data for the current sample (shape: (num_timesteps,))\n",
    "            lead_data = ecg_data[sample_idx, :, lead_idx]\n",
    "\n",
    "            # Calculate the mean and standard deviation for the lead within the sample\n",
    "            mean = np.mean(lead_data)\n",
    "            std = np.std(lead_data)\n",
    "\n",
    "            # Apply Z-normalization to the lead\n",
    "            if std != 0:  # Avoid division by zero\n",
    "                normalized_lead_data = (lead_data - mean) / std\n",
    "            else:\n",
    "                normalized_lead_data = np.zeros_like(lead_data)  # Handle constant signals\n",
    "\n",
    "            # Store the normalized lead data back into the array\n",
    "            normalized_ecg_data[sample_idx, :, lead_idx] = normalized_lead_data\n",
    "\n",
    "    return normalized_ecg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_scale_sample_lead_wise(ecg_data):\n",
    "    \"\"\"\n",
    "    Apply Robust Scaling sample-wise and lead-wise to an ECG signal array.\n",
    "\n",
    "    Parameters:\n",
    "        ecg_data (numpy.ndarray): Input ECG data array of shape (num_samples, num_timesteps, num_leads).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Robust-scaled ECG data array of the same shape as input.\n",
    "    \"\"\"\n",
    "    # Initialize an array to store the scaled data\n",
    "    scaled_ecg_data = np.zeros_like(ecg_data)\n",
    "\n",
    "    # Loop through each sample\n",
    "    for sample_idx in range(ecg_data.shape[0]):\n",
    "        # Loop through each lead (signal)\n",
    "        for lead_idx in range(ecg_data.shape[2]):\n",
    "            # Extract the lead data for the current sample (shape: (num_timesteps,))\n",
    "            lead_data = ecg_data[sample_idx, :, lead_idx]\n",
    "\n",
    "            # Calculate the median and IQR for the lead within the sample\n",
    "            median = np.median(lead_data)\n",
    "            q1 = np.percentile(lead_data, 25)  # 25th percentile (Q1)\n",
    "            q3 = np.percentile(lead_data, 75)  # 75th percentile (Q3)\n",
    "            iqr = q3 - q1  # Interquartile range\n",
    "\n",
    "            # Apply Robust Scaling to the lead\n",
    "            if iqr != 0:  # Avoid division by zero\n",
    "                scaled_lead_data = (lead_data - median) / iqr\n",
    "            else:\n",
    "                scaled_lead_data = np.zeros_like(lead_data)  # Handle constant signals\n",
    "\n",
    "            # Store the scaled lead data back into the array\n",
    "            scaled_ecg_data[sample_idx, :, lead_idx] = scaled_lead_data\n",
    "\n",
    "    return scaled_ecg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale_sample_lead_wise(ecg_data):\n",
    "    \"\"\"\n",
    "    Apply Min-Max Scaling sample-wise and lead-wise to an ECG signal array, scaling data to the range [-1, 1].\n",
    "\n",
    "    Parameters:\n",
    "        ecg_data (numpy.ndarray): Input ECG data array of shape (num_samples, num_timesteps, num_leads).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Min-Max scaled ECG data array of the same shape as input, with values scaled to [-1, 1].\n",
    "    \"\"\"\n",
    "    # Initialize an array to store the scaled data\n",
    "    scaled_ecg_data = np.zeros_like(ecg_data)\n",
    "\n",
    "    # Loop through each sample\n",
    "    for sample_idx in range(ecg_data.shape[0]):\n",
    "        # Loop through each lead (signal)\n",
    "        for lead_idx in range(ecg_data.shape[2]):\n",
    "            # Extract the lead data for the current sample (shape: (num_timesteps,))\n",
    "            lead_data = ecg_data[sample_idx, :, lead_idx]\n",
    "\n",
    "            # Calculate the min and max for the lead within the sample\n",
    "            lead_min = np.min(lead_data)\n",
    "            lead_max = np.max(lead_data)\n",
    "\n",
    "            # Apply Min-Max Scaling to the lead and scale to [-1, 1]\n",
    "            if lead_max != lead_min:  # Avoid division by zero\n",
    "                scaled_lead_data = 2 * (lead_data - lead_min) / (lead_max - lead_min) - 1\n",
    "            else:\n",
    "                scaled_lead_data = np.zeros_like(lead_data)  # Handle constant signals\n",
    "\n",
    "            # Store the scaled lead data back into the array\n",
    "            scaled_ecg_data[sample_idx, :, lead_idx] = scaled_lead_data\n",
    "\n",
    "    return scaled_ecg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale_centered_to_0(ecg_data):\n",
    "    \"\"\"\n",
    "    Apply Min-Max Scaling to ECG data to be in the range [-1, 1], while preserving the baseline around 0.\n",
    "    \n",
    "    This is done by centering the data around 0 (using median or mean) and then scaling.\n",
    "    \n",
    "    Parameters:\n",
    "        ecg_data (numpy.ndarray): Input ECG data array of shape (num_samples, num_timesteps, num_leads).\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Min-Max scaled ECG data array of the same shape as input, with values scaled to [-1, 1] and centered around 0.\n",
    "    \"\"\"\n",
    "    # Initialize an array to store the scaled data\n",
    "    scaled_ecg_data = np.zeros_like(ecg_data)\n",
    "\n",
    "    # Loop through each sample\n",
    "    for sample_idx in range(ecg_data.shape[0]):\n",
    "        # Loop through each lead (signal)\n",
    "        for lead_idx in range(ecg_data.shape[2]):\n",
    "            # Extract the lead data for the current sample (shape: (num_timesteps,))\n",
    "            lead_data = ecg_data[sample_idx, :, lead_idx]\n",
    "\n",
    "            # Center the data by subtracting the median (or mean) for each lead to preserve baseline around 0\n",
    "            median = np.median(lead_data)  # You can use np.mean if preferred\n",
    "            centered_data = lead_data - median\n",
    "\n",
    "            # Find the max absolute value of the centered data\n",
    "            max_abs_value = np.max(np.abs(centered_data))\n",
    "\n",
    "            # Apply Min-Max Scaling to the centered data, ensuring the range is [-1, 1]\n",
    "            if max_abs_value != 0:  # Avoid division by zero\n",
    "                scaled_lead_data = centered_data / max_abs_value\n",
    "            else:\n",
    "                scaled_lead_data = np.zeros_like(centered_data)  # Handle constant signals\n",
    "\n",
    "            # Store the scaled lead data back into the array\n",
    "            scaled_ecg_data[sample_idx, :, lead_idx] = scaled_lead_data\n",
    "\n",
    "    return scaled_ecg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Y_train_scaled = min_max_scale_centered_to_0(Y_train_detrend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Y_test_scaled = min_max_scale_centered_to_0(Y_test_detrend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Y_valid_scaled = min_max_scale_centered_to_0(Y_valid_detrend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ecg(Y_train_scaled[0], title='Train ECG Sample Scaled', y_title='Normalized scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ecg(Y_valid_scaled[0], title='Validation ECG Sample Scaled', y_title='Normalized scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ecg(Y_test_scaled[0], title='Test ECG Sample Scaled', y_title='Normalized scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import FontProperties\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2_ecgs(ecg_1: np.array, title_ecg_1: str, ecg_2: np.array, title_ecg_2: str, title: str = '2 ECG signals', sampling_rate: int = 1000, render_title = True):\n",
    "    fig, axs = plt.subplots(12, 1, figsize=(14, 15))\n",
    "    # Standard lead names for 12-lead ECG\n",
    "    lead_names = ['I', 'II', 'III', 'aVL', 'aVR', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.plot(ecg_1[:,i], alpha=0.85, linewidth=0.85, color='indigo', label=title_ecg_1)\n",
    "        ax.plot(ecg_2[:,i], alpha=0.75, linewidth=0.85, color='coral', label=title_ecg_2) \n",
    "        ax.set_facecolor('white')\n",
    "        ax.spines['top'].set_color('gray')\n",
    "        ax.spines['right'].set_color('gray')\n",
    "        ax.spines['bottom'].set_color('gray')\n",
    "        ax.spines['left'].set_color('gray')\n",
    "        ax.grid(True, color='gray', alpha=0.65, linewidth=0.5, linestyle='dashed')\n",
    "\n",
    "        # Set xlim to prevent offset and remove any gap\n",
    "        ax.set_xlim([0, ecg_1.shape[0]])\n",
    "\n",
    "        # Increase the font size of ticks\n",
    "        ax.tick_params(axis='both', which='major', labelsize=13)  # You can adjust this value as needed\n",
    "\n",
    "        ax.set_xticks(np.arange(0, ecg_1.shape[0] + 1, ecg_1.shape[0] / 10))  # Add more ticks\n",
    "        if not i == len(ecg_1[1]) - 1:\n",
    "            ax.set_xticklabels([])  # Remove x-axis labels\n",
    "\n",
    "        # Compute min and max\n",
    "        y_min, y_max = np.min(ecg_1[:, i]), np.max(ecg_1[:, i])\n",
    "        ax.set_ylim([y_min, y_max])  # Set y-axis limits\n",
    "\n",
    "        # Move lead annotations to the right\n",
    "        ax.text(ecg_1.shape[0] * 1.025, (y_max + y_min) / 2, lead_names[i], \n",
    "                fontsize=15, color='black', ha='center', va='center')\n",
    "        \n",
    "    fig.text(0.5, 0.00, f'Time (in samples)', ha='center', va='center', fontsize=16)\n",
    "    fig.text(0.0, 0.5, 'Normalized scale', ha='center', va='center', rotation='vertical', fontsize=16)\n",
    "    # Add a main title\n",
    "    if render_title:\n",
    "        plt.suptitle(title, fontsize=25, y=0.99)\n",
    "    # Add legend at the top\n",
    "    fig.legend([title_ecg_1, title_ecg_2], loc='upper right', bbox_to_anchor=(1.0, 1.0), ncol=2, prop=FontProperties(size=16))\n",
    "\n",
    "    # Remove space between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ecgmentations as E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = E.Sequential([\n",
    "    E.GaussNoise(p=1.0,\n",
    "                variance=0.002 # mV\n",
    "                )\n",
    "])\n",
    "transformed_ecg = transform(ecg=Y_train_scaled[0])['ecg']\n",
    "plot_2_ecgs(Y_train_scaled[0], 'Original', transformed_ecg, 'Gaussian Noise', 'Augmentation Gaussian Noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = E.Sequential([\n",
    "    E.SinePulse(p=1.0,\n",
    "                ecg_frequency=100., # 100 Hz \n",
    "                amplitude_limit=0.2, # mV\n",
    "                )\n",
    "])\n",
    "transformed_ecg = transform(ecg=Y_train_scaled[0])['ecg']\n",
    "plot_2_ecgs(Y_train_scaled[0], 'Original', transformed_ecg, 'Sine Pulse', 'Augmentation Sine Pulse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = E.Sequential([\n",
    "    E.RespirationNoise(p=1.0,\n",
    "                ecg_frequency=100., # 100Hz\n",
    "                breathing_rate_range=(12, 18), # breathing rate range in bpm\n",
    "                amplitude_limit=0.2, # mV\n",
    "                )\n",
    "])\n",
    "transformed_ecg = transform(ecg=Y_train_scaled[0])['ecg']\n",
    "plot_2_ecgs(Y_train_scaled[0], 'Original', transformed_ecg, 'Breathing', 'Augmentation Breathing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = E.Sequential([\n",
    "    E.AmplitudeScale(p=1.0,\n",
    "                    scaling_range=(-0.2,0.2)\n",
    "    )\n",
    "])\n",
    "transformed_ecg = transform(ecg=Y_train_scaled[0])['ecg']\n",
    "plot_2_ecgs(Y_train_scaled[0], 'Original', transformed_ecg, 'Amplitute', 'Augmentation Amplitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = E.Sequential([\n",
    "    E.TimeShift(p=1.0,\n",
    "                shift_limit=0.02\n",
    "    )\n",
    "])\n",
    "transformed_ecg = transform(ecg=Y_train_scaled[0])['ecg']\n",
    "plot_2_ecgs(Y_train_scaled[0], 'Original', transformed_ecg, 'Shifted', 'Augmentation Time Shift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = E.Sequential([\n",
    "    E.RandomTimeWrap(p=1.0,\n",
    "                num_steps=4,\n",
    "                wrap_limit=0.1,\n",
    "    )\n",
    "])\n",
    "transformed_ecg = transform(ecg=Y_train_scaled[0])['ecg']\n",
    "plot_2_ecgs(Y_train_scaled[0], 'Original', transformed_ecg, 'Warped', 'Augmentation Time Warp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form augmentation pipeline\n",
    "transform = E.Sequential([\n",
    "        E.TimeShift(p=0.5,\n",
    "                shift_limit=0.03),\n",
    "        E.AmplitudeScale(p=0.5,\n",
    "                    scaling_range=(-0.2,0.2)),\n",
    "        E.GaussNoise(p=0.5,\n",
    "                variance=0.001),\n",
    "        E.OneOf([\n",
    "            E.SinePulse(p=0.5,\n",
    "                ecg_frequency=100., # 100 Hz \n",
    "                amplitude_limit=0.2),\n",
    "            E.RespirationNoise(p=0.5,\n",
    "                ecg_frequency=100., # 100Hz\n",
    "                breathing_rate_range=(12, 18), # breathing rate range in bpm\n",
    "                amplitude_limit=0.2)\n",
    "                ])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_ecg = transform(ecg=Y_train_scaled[0])['ecg']\n",
    "plot_2_ecgs(Y_train_scaled[0], 'Original', transformed_ecg, 'Augmented', 'Augmentation Pipeline', render_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_ecg = transform(ecg=Y_train_scaled[0])['ecg']\n",
    "plot_2_ecgs(Y_train_scaled[0], 'Original', transformed_ecg, 'Augmented', 'Augmentation Pipeline', render_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_ecg = transform(ecg=Y_train_scaled[0])['ecg']\n",
    "plot_2_ecgs(Y_train_scaled[0], 'Original', transformed_ecg, 'Augmented', 'Augmentation Pipeline', render_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from models import SignalCNNTransformer\n",
    "import ecgmentations\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "class SignalCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 signal_channels=12,\n",
    "                 initial_conv_kernel_size=11,\n",
    "                 conv_kernel_size = 7,\n",
    "                 conv_filters=16,\n",
    "                 downscale_cnn_blocks=2,\n",
    "                 number_of_classes=2,\n",
    "                 add_cam=False) -> None:\n",
    "            super(SignalCNN, self).__init__()\n",
    "            self.downscale_cnn_blocks = downscale_cnn_blocks\n",
    "            self.conv_filters = conv_filters\n",
    "            self.add_cam = add_cam\n",
    "            # in the input do one convolutional operation with\n",
    "            self.pre_cnn_extractor =  nn.Sequential(\n",
    "                nn.Conv1d(in_channels=signal_channels,\n",
    "                          out_channels=conv_filters,\n",
    "                          kernel_size=initial_conv_kernel_size,\n",
    "                          padding='same'),\n",
    "                nn.BatchNorm1d(conv_filters),\n",
    "                nn.LeakyReLU(negative_slope=0.1),\n",
    "                nn.MaxPool1d(2, stride=2))\n",
    "            # initially do feature extraction with CNN layers\n",
    "            self.cnn_extractor = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                nn.Conv1d(in_channels=conv_filters if i == 0 else 2 ** (i - 1) * conv_filters,\n",
    "                          out_channels=2 ** i * conv_filters,\n",
    "                          kernel_size=conv_kernel_size,\n",
    "                          padding='same'),\n",
    "                nn.BatchNorm1d(2 ** i * conv_filters),\n",
    "                nn.LeakyReLU(negative_slope=0.1),\n",
    "                nn.MaxPool1d(2, stride=2)\n",
    "            )\n",
    "            for i in range(downscale_cnn_blocks)\n",
    "            ])\n",
    "            # lets put the last convolution with separate operation for easier operation access in cam\n",
    "            self.last_conv = nn.Conv1d(in_channels=2 ** (downscale_cnn_blocks - 1) * conv_filters,\n",
    "                          out_channels=2 ** downscale_cnn_blocks * conv_filters,\n",
    "                          kernel_size=conv_kernel_size,\n",
    "                          padding='same')\n",
    "            self.last_batch_norm = nn.BatchNorm1d(2 ** downscale_cnn_blocks * conv_filters)\n",
    "            self.last_activation = nn.LeakyReLU(negative_slope=0.1)\n",
    "            self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "            number_of_classes = number_of_classes if number_of_classes > 2 else 1 # two class is sigmoid\n",
    "            self.pred_layer = nn.Linear(2 ** (downscale_cnn_blocks) * conv_filters, number_of_classes)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "    def forward(self, x_input):\n",
    "        # build CNN part of network\n",
    "        x = self.pre_cnn_extractor(x_input)\n",
    "        for layer in self.cnn_extractor:\n",
    "            x = layer(x)\n",
    "        features = self.last_conv(x)\n",
    "        x = self.last_batch_norm(features)\n",
    "        x = self.last_activation(x)\n",
    "        x_classification = self.global_avg_pool(x)\n",
    "        x_classification = x_classification.view(x_classification.size(0), -1)  # Flatten the output\n",
    "        x_classification = self.pred_layer(x_classification)\n",
    "        x_classification = self.sigmoid(x_classification)\n",
    "        if self.add_cam:\n",
    "            #print(features.shape)\n",
    "            weights = self.pred_layer.weight.data.unsqueeze(-1)\n",
    "            features_reshaped = features.permute(0, 2, 1)\n",
    "            result = torch.matmul(features_reshaped, weights).unsqueeze(0)\n",
    "            # Interpolate between the tensors using linear interpolation\n",
    "            interpolated_result = F.interpolate(result,\n",
    "                                                size=(x_input.shape[2], 1),\n",
    "                                                mode='bilinear',\n",
    "                                                align_corners=False)\n",
    "            interpolated_result = (interpolated_result - interpolated_result.min()) / (interpolated_result.max() - interpolated_result.min())\n",
    "            return x_classification, interpolated_result.squeeze(0).squeeze(-1) # remove first and last dims\n",
    "        return x_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_metrics(op, labels):\n",
    "    # Detach from computational graph and convert to numpy\n",
    "    op_probs = torch.detach(op).numpy()\n",
    "    \n",
    "    # Convert probabilities to predicted class labels (0 or 1)\n",
    "    op_labels = (op_probs > 0.5).astype(int)\n",
    "    \n",
    "    # Convert labels to numpy array if it's a tensor\n",
    "    labels = labels.numpy() if isinstance(labels, torch.Tensor) else labels\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(op_labels, labels)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset loader\n",
    "class PTBLoader(Dataset):\n",
    "    def __init__(self, signal_array: np.array,\n",
    "                 labels: np.array,\n",
    "                 augmentation: ecgmentations.core.compositions.Sequential = None) -> None:\n",
    "        super().__init__()\n",
    "        self.data: np.array = signal_array\n",
    "        self.labels: np.array = labels\n",
    "        self.augmentation: ecgmentations.core.compositions.Sequential = augmentation\n",
    "        print(f'Data loader with {self.data.shape[0]} elements')\n",
    "        print(f'Data loader with {self.labels.shape[0]} labels')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        try:\n",
    "            ecg = self.data[idx]\n",
    "            if not self.augmentation is None:\n",
    "                ecg_ = self.augmentation(ecg=ecg)['ecg']\n",
    "            else:\n",
    "                ecg_ = ecg\n",
    "            # Convert one-hot encoded array to integer\n",
    "            # label = np.argmax(self.labels[idx])\n",
    "            #if label > 0:\n",
    "            #   label = 1\n",
    "            label = 0 if np.all(self.labels[idx] == np.array([1, 0, 0, 0, 0])) else 1 # Only norm with any other\n",
    "            data_final = torch.Tensor(ecg_.T) # transpose\n",
    "        except BaseException:\n",
    "            print(f'Failed to fetch index {idx}')\n",
    "            return None, None\n",
    "        return data_final, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# epoch to train\n",
    "epochs = 25\n",
    "batch_size = 128\n",
    "# train - test\n",
    "train_dataset = PTBLoader(Y_train_scaled, Z_train.to_numpy(), transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = PTBLoader(Y_valid_scaled, Z_valid.to_numpy())\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output path for model\n",
    "signal_model_output_path = 'model_0.pt'\n",
    "# create neural model\n",
    "ecg_leads_count = 12\n",
    "model = SignalCNN(ecg_leads_count, conv_filters=16, number_of_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),lr=0.0005)\n",
    "# Learning rate scheduling [automatically reduce]\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       mode='min',\n",
    "                                                       factor=0.5,\n",
    "                                                       patience=8,\n",
    "                                                       threshold=0.0005,\n",
    "                                                       threshold_mode='rel',\n",
    "                                                       cooldown=0,\n",
    "                                                       min_lr=0,\n",
    "                                                       eps=1e-08)\n",
    "#Loss function\n",
    "loss = torch.nn.BCELoss()\n",
    "# best test accuraccy for training\n",
    "best_accuracy = 0.0\n",
    "best_accuracy_epoch = -1\n",
    "\n",
    "statistics = {'train_loss': [],\n",
    "              'train_acc': [],\n",
    "              'test_loss': [],\n",
    "              'test_acc': [],\n",
    "              'epoch': [],\n",
    "              'lr': [],\n",
    "              'best_acc': 0.0,\n",
    "              'best_acc_epoch': -1}\n",
    "    \n",
    "for epoch_index in range(epochs):\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc_avg = 0.0\n",
    "    total_loss_avg = 0.0\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        signal,label=data\n",
    "        optimizer.zero_grad()\n",
    "        output=model(signal)\n",
    "        loss_=loss(output.squeeze(),label.float())\n",
    "        loss_.backward()\n",
    "        optimizer.step()\n",
    "        acc_score=acc_metrics(output.squeeze(), label.float())\n",
    "        # accumulate accuracy and loss\n",
    "        total_acc += acc_score\n",
    "        total_loss += loss_.item()\n",
    "        total_acc_avg = total_acc if idx == 0 else total_acc / float(idx + 1)\n",
    "        total_loss_avg = total_loss if idx == 0 else total_loss / float(idx + 1)\n",
    "        print('[Training] Epoch: {}/{} Mini Batch: {}/{} Accuracy : {:4f}, Loss: {:4f}'.format(epoch_index + 1, epochs, idx, len(train_loader), total_acc_avg, total_loss_avg), end='\\r')\n",
    "    # testing stage\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_val_acc_avg = 0.0\n",
    "    total_val_loss_avg = 0.0\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(valid_loader):\n",
    "            signal,label=data\n",
    "            output=model(signal)\n",
    "            acc_score=acc_metrics(output.squeeze(),label.float())\n",
    "            loss_=loss(output.squeeze(),label.float())\n",
    "            # accumulate accuracy and loss\n",
    "            total_acc += acc_score\n",
    "            total_loss += loss_.item()\n",
    "            total_val_acc_avg = total_acc if idx == 0 else total_acc / float(idx + 1)\n",
    "            total_val_loss_avg = total_loss if idx == 0 else total_loss / float(idx + 1)\n",
    "            print('[Testing] Epoch: {}/{} Mini Batch: {}/{} Accuracy : {:4f}, Loss: {:4f}'.format(epoch_index + 1, epochs, idx, len(valid_loader), total_acc_avg, total_loss_avg), end='\\r')\n",
    "        current_accuracy = (total_acc / len(valid_loader))\n",
    "        # let scheduler do reduction in case validation score does not change\n",
    "        scheduler.step(current_accuracy)\n",
    "        if current_accuracy > best_accuracy:\n",
    "            print(f'New best weights found with accuracy {current_accuracy} at epoch {epoch_index}. Saving to {signal_model_output_path}...')\n",
    "            torch.save(model.state_dict(), signal_model_output_path)\n",
    "            best_accuracy = current_accuracy\n",
    "            best_accuracy_epoch = epoch_index\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch {epoch_index} learning rate: {lr}')\n",
    "    # save parameters\n",
    "    statistics['train_loss'].append(total_loss_avg)\n",
    "    statistics['train_acc'].append(total_acc_avg)\n",
    "    statistics['test_loss'].append(total_val_loss_avg)\n",
    "    statistics['test_acc'].append(total_val_acc_avg)\n",
    "    statistics['epoch'].append(epoch_index)\n",
    "    statistics['lr'].append(lr)\n",
    "    statistics['best_acc'] = best_accuracy\n",
    "    statistics['best_acc_epoch'] = best_accuracy_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "def plot_train_metrics(metrics):\n",
    "    epochs = range(len(metrics['train_loss']))\n",
    "    best_acc_epoch = metrics['best_acc_epoch']\n",
    "    best_acc = metrics['best_acc']\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # First subplot\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    ax1.set_facecolor('white')\n",
    "    ax1.spines['top'].set_color('gray')\n",
    "    ax1.spines['right'].set_color('gray')\n",
    "    ax1.spines['bottom'].set_color('gray')\n",
    "    ax1.spines['left'].set_color('gray')\n",
    "    ax1.grid(True, color='gray', alpha=0.65, linewidth=0.5, linestyle='dashed')\n",
    "    ax1.set_xticks(np.arange(0, len(metrics['train_loss']) + 1, len(metrics['train_loss']) / 10).astype(int))\n",
    "    \n",
    "    ax1.plot(epochs, metrics['train_loss'], 'b--', label='Training loss')\n",
    "    ax1.plot(epochs, metrics['test_loss'], 'r--', label='Test loss')\n",
    "    ax1.plot(epochs, metrics['train_acc'], 'b-', label='Training accuracy')\n",
    "    ax1.plot(epochs, metrics['test_acc'], 'r-', label='Test accuracy')\n",
    "    ax1.plot(best_acc_epoch, best_acc, 'g*', markersize=10, label=f'Best accuracy: {best_acc:.4f}')  # Mark best accuracy with a star\n",
    "    ax1.axvline(x=best_acc_epoch, color='gray', linestyle='--')  # Vertical line at best accuracy epoch\n",
    "    ax1.set_title('Training and Test Metrics')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Metrics')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Annotate best accuracy\n",
    "    #ax1.annotate(f'Best accuracy: {best_acc:.4f}', xy=(best_acc_epoch, best_acc), xytext=(best_acc_epoch+5, best_acc-0.02),\n",
    "    #             arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "    \n",
    "    # Second subplot\n",
    "    ax2 = plt.subplot(2, 1, 2)\n",
    "    ax2.set_facecolor('white')\n",
    "    ax2.spines['top'].set_color('gray')\n",
    "    ax2.spines['right'].set_color('gray')\n",
    "    ax2.spines['bottom'].set_color('gray')\n",
    "    ax2.spines['left'].set_color('gray')\n",
    "    ax2.grid(True, color='gray', alpha=0.65, linewidth=0.5, linestyle='dashed')\n",
    "    ax2.set_xticks(np.arange(0, len(metrics['train_loss']) + 1, len(metrics['train_loss']) / 10).astype(int))\n",
    "    \n",
    "    ax2.plot(epochs, metrics['lr'], 'g-', label='Learning rate')\n",
    "    ax2.set_title('Learning Rate Scheduling')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "    ax2.legend()  # Add legend to the second subplot\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_metrics(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PTBLoader(Y_test_scaled, Z_test.to_numpy())\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_jet_colormap(data):\n",
    "    \"\"\"\n",
    "    Plot data using the jet colormap.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): Input data array in the range [0, 1].\n",
    "    \"\"\"\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(8, 1))\n",
    "\n",
    "    # Plot the data using the jet colormap\n",
    "    ax.imshow(data.reshape(1, -1), cmap='jet', aspect='auto')\n",
    "\n",
    "    # Remove y-axis ticks and labels\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    # Set x-axis ticks and labels\n",
    "    ax.set_xticks([0, len(data) // 2, len(data) - 1])\n",
    "    ax.set_xticklabels(['0', str(len(data) // 2), str(len(data) - 1)])\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Data Points')\n",
    "    plt.title('Jet Colormap')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of your model\n",
    "ecg_leads_count = 12\n",
    "model = SignalCNN(ecg_leads_count, conv_filters=16, number_of_classes=2, add_cam=True)\n",
    "\n",
    "# Load the trained parameters from the .pt file\n",
    "model_path = 'model_0.pt'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def plot_confusion_matrix(y_true_list, y_pred_list, classes, title):\n",
    "    # Build confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    normalized_cf_matrix = cf_matrix / np.sum(cf_matrix, axis=1)[:, None]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_cm = pd.DataFrame(normalized_cf_matrix, index=classes, columns=classes)\n",
    "\n",
    "    # Set up figure size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Generate heatmap with increased annotation size\n",
    "    heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f', cmap='YlGnBu', \n",
    "                          annot_kws={'size': 0}, cbar_kws={'shrink': 1.0})\n",
    "\n",
    "    # Increase font size of axis labels and tick labels\n",
    "    heatmap.set_xticklabels(heatmap.get_xticklabels(), fontsize=25)\n",
    "    heatmap.set_yticklabels(heatmap.get_yticklabels(), fontsize=25)\n",
    "    \n",
    "    # Increase font size of colorbar tick labels\n",
    "    cbar = heatmap.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=26)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true_list, y_pred_list)\n",
    "    accuracy_percent = accuracy * 100.0\n",
    "\n",
    "    # Add title with increased font size\n",
    "    plt.title(f'{title} (Accuracy={accuracy_percent:.2f}%)', fontsize=26)\n",
    "\n",
    "    # Loop through each cell in the heatmap and add element counts with semi-transparent background\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            count = cf_matrix[i, j]\n",
    "            value = df_cm.iloc[i, j]\n",
    "            text = f'{value*100:.2f}%\\n({count})'\n",
    "\n",
    "            # Use a contrasting text color based on the cell value\n",
    "            if value > 0.5:\n",
    "                text_color = 'white'        # Light text for higher values\n",
    "            else:\n",
    "                text_color = 'black'        # Dark text for lower values\n",
    "\n",
    "            # Add text annotation with a semi-transparent background\n",
    "            heatmap.text(j + 0.5, i + 0.5, text, ha='center', va='center', \n",
    "                         fontsize=34, color=text_color,\n",
    "                         bbox=dict(facecolor='white', alpha=0.0, edgecolor='none', boxstyle='round,pad=0.1'))\n",
    "\n",
    "    # Display plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_jet_colormap(data, ax, y_min, y_max):\n",
    "    \"\"\"\n",
    "    Plot data using the jet colormap on a given axis.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): Input data array in the range [0, 1].\n",
    "        ax (matplotlib.axes.Axes): Axis to plot the colormap on.\n",
    "    \"\"\"\n",
    "    # Plot the data using the jet colormap\n",
    "    ax.imshow(data.reshape(1, -1), cmap='jet', aspect='auto', alpha=0.4, extent=[0, data.shape[1], y_min, y_max])\n",
    "    \n",
    "def plot_ecg_high(ecg: np.array, title: str = 'ECG signal', data_1000: np.array = None):\n",
    "    if data_1000 is None:\n",
    "        raise ValueError(\"Input 'data_1000' is required.\")\n",
    "    \n",
    "    if ecg.shape != (1000, 12):\n",
    "        raise ValueError(\"Input 'ecg' must be of shape (1000, 12)\")\n",
    "\n",
    "    lead_names = ['I', 'II', 'III', 'aVL', 'aVR', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "    \n",
    "    fig, axs = plt.subplots(12, 1, figsize=(14, 15))\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        # Overlay the ECG signal on top of the colormap\n",
    "        ax.plot(ecg[:,i], alpha=0.95, linewidth=0.95, color='black')\n",
    "        \n",
    "        ax.set_facecolor('white')\n",
    "        ax.spines['top'].set_color('gray')\n",
    "        ax.spines['right'].set_color('gray')\n",
    "        ax.spines['bottom'].set_color('gray')\n",
    "        ax.spines['left'].set_color('gray')\n",
    "        \n",
    "        ax.grid(True, color='gray', alpha=0.65, linewidth=0.5, linestyle='dashed')\n",
    "        ax.set_xticks(np.arange(0, ecg.shape[0] + 1, ecg.shape[0] / 10))  # Add more ticks\n",
    "        if not i == len(ecg[1]) - 1:\n",
    "            ax.set_xticklabels([])  # Remove x-axis labels\n",
    "        ylim = ax.get_ylim()\n",
    "\n",
    "        # Compute min and max\n",
    "        y_min, y_max = np.min(ecg[:, i]), np.max(ecg[:, i])\n",
    "        ax.set_ylim([y_min, y_max])  # Set y-axis limits\n",
    "\n",
    "        # Move lead annotations to the right\n",
    "        ax.text(ecg.shape[0] * 1.025, (y_max + y_min) / 2, lead_names[i], \n",
    "                fontsize=15, color='black', ha='center', va='center')\n",
    "        \n",
    "        # Invert y-axis labels\n",
    "        #ax.set_yticklabels(reversed(ax.get_yticklabels()))\n",
    "        # Plot the colormap\n",
    "        plot_jet_colormap(data_1000, ax, ylim[0], ylim[1])\n",
    "        # Increase y-axis tick size\n",
    "        ax.tick_params(axis='y', labelsize=16)  # Adjust the labelsize as needed\n",
    "        ax.tick_params(axis='x', labelsize=16) \n",
    "            \n",
    "    fig.text(0.5, 0.00, 'Time (in samples)', ha='center', va='center', fontsize=16)\n",
    "    fig.text(0.00, 0.5, 'Normalized scale', ha='center', va='center', rotation='vertical', fontsize=16)\n",
    "    plt.suptitle(title, fontsize=20, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_visualize = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Normal vs Abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Evaluate\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # Store probabilities for ROC curve\n",
    "threshold = 0.5\n",
    "counter = 0\n",
    "\n",
    "for idx, data in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "    signal, label = data\n",
    "    \n",
    "    output, cam = model(signal)  # Feed Network\n",
    "    output = output.squeeze()\n",
    "    \n",
    "    op_prob = torch.detach(output).numpy().item()  # Convert tensor to numpy\n",
    "    y_prob.append(op_prob)  # Store probability\n",
    "\n",
    "    # Convert probabilities to predicted class labels (0 or 1)\n",
    "    op_label = 1 if op_prob > threshold else 0\n",
    "    y_pred.append(op_label)\n",
    "\n",
    "    label_ = label.data.cpu().numpy().item()\n",
    "    y_true.append(label_)  # Save Truth\n",
    "\n",
    "    # View abnormal ECGs\n",
    "    if op_label == 1 and label_ == 1 and counter < sample_to_visualize:\n",
    "        cam_numpy = cam.detach().numpy()\n",
    "        signal_numpy = signal.detach().numpy().squeeze(0)\n",
    "        plot_ecg_high(signal_numpy.T, title='Abnormal', data_1000=cam_numpy)\n",
    "        counter += 1\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Precision, Recall, and F1 Score calculations\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Define classes\n",
    "classes = ('Normal', 'Diseased')\n",
    "title = f'Normal and Diseased'\n",
    "plot_confusion_matrix(y_true, y_pred, classes, title)\n",
    "\n",
    "# Calculate and plot ROC curve, AUC\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"\\nAUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Create the ROC plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# ROC curve\n",
    "plt.plot(fpr, tpr, color='darkorchid', linewidth=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Transparent fill under the ROC curve\n",
    "plt.fill_between(fpr, tpr, alpha=0.05, color='darkorchid')\n",
    "\n",
    "# Diagonal line\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=2)  \n",
    "\n",
    "# Titles and labels\n",
    "plt.xlabel('False Positive Rate', fontsize=19)\n",
    "plt.ylabel('True Positive Rate', fontsize=19)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=24)\n",
    "\n",
    "# Stronger grid visibility\n",
    "plt.grid(True, linestyle='--', linewidth=0.7, alpha=0.8, color='gray')\n",
    "\n",
    "# Ensure all axes and borders are visible\n",
    "plt.tick_params(axis='both', which='major', labelsize=19)\n",
    "plt.gca().spines['top'].set_visible(True)\n",
    "plt.gca().spines['right'].set_visible(True)\n",
    "plt.gca().spines['left'].set_visible(True)\n",
    "plt.gca().spines['bottom'].set_visible(True)\n",
    "\n",
    "# Legend settings\n",
    "plt.legend(loc='lower right', fontsize=18, frameon=True)\n",
    "\n",
    "# White background\n",
    "plt.gca().set_facecolor('white')\n",
    "plt.gcf().set_facecolor('white')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal vs Myocardial Infarction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick Myocardial Infarction and normal\n",
    "labels = Z_test.to_numpy()\n",
    "conditions = (\n",
    "    ((labels[:, 0] == 1) & (labels[:, 1:] == 0).all(axis=1)) |  \n",
    "    (labels[:, 1] == 1) \n",
    ")\n",
    "#conditions = ((labels == [1, 0, 0, 0, 0]).all(axis=1)) | ((labels == [0, 1, 0, 0, 0]).all(axis=1))\n",
    "indices = np.where(conditions)[0]\n",
    "Z_test_ = labels[indices]\n",
    "Y_test_ = Y_test_scaled[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PTBLoader(Y_test_, Z_test_)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "# evaluate\n",
    "y_pred = []\n",
    "y_true = []\n",
    "threshold = 0.5\n",
    "counter = 0\n",
    "for idx, data in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "    signal, label = data\n",
    "    output, cam = model(signal) # Feed Network\n",
    "    output = output.squeeze()\n",
    "    op_prob = torch.detach(output).numpy().item()\n",
    "    # Convert probabilities to predicted class labels (0 or 1)\n",
    "    op_label = 1 if op_prob > threshold else 0\n",
    "    y_pred.append(op_label)\n",
    "    label_ = label.data.cpu().numpy().item()\n",
    "    y_true.append(label_) # Save Truth\n",
    "    # lets try to view few abnormal ecgs with XAI\n",
    "    if op_label == 1 and label_ == 1 and counter < sample_to_visualize:\n",
    "        cam_numpy = cam.detach().numpy()\n",
    "        #plot_jet_colormap(cam_numpy)\n",
    "        signal_numpy = signal.detach().numpy().squeeze(0)\n",
    "        plot_ecg_high(signal_numpy.T, title = 'Myocardial Infarction', data_1000=cam_numpy)\n",
    "        counter += 1\n",
    "# Define classes\n",
    "classes = ('Norm', 'Myocardial\\n Infarction')\n",
    "title = f'Confusion Matrix - Norm and Myocardial Infarction\\n'\n",
    "plot_confusion_matrix(y_true, y_pred, classes, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal vs ST/T Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick ST/T Change and normal\n",
    "labels = Z_test.to_numpy()\n",
    "#conditions = ((labels == [1, 0, 0, 0, 0]).all(axis=1)) | ((labels == [0, 0, 1, 0, 0]).all(axis=1))\n",
    "conditions = (\n",
    "    ((labels[:, 0] == 1) & (labels[:, 1:] == 0).all(axis=1)) |  \n",
    "    (labels[:, 2] == 1) \n",
    ")\n",
    "indices = np.where(conditions)[0]\n",
    "Z_test_ = labels[indices]\n",
    "Y_test_ = Y_test_scaled[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PTBLoader(Y_test_, Z_test_)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "# evaluate\n",
    "y_pred = []\n",
    "y_true = []\n",
    "threshold = 0.5\n",
    "counter = 0\n",
    "for idx, data in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "    signal, label = data\n",
    "    output, cam = model(signal) # Feed Network\n",
    "    output = output.squeeze()\n",
    "    op_prob = torch.detach(output).numpy().item()\n",
    "    # Convert probabilities to predicted class labels (0 or 1)\n",
    "    op_label = 1 if op_prob > threshold else 0\n",
    "    y_pred.append(op_label)\n",
    "    label_ = label.data.cpu().numpy().item()\n",
    "    y_true.append(label_) # Save Truth\n",
    "    # lets try to view few abnormal ecgs with XAI\n",
    "    if op_label == 1 and label_ == 1 and counter < sample_to_visualize:\n",
    "        cam_numpy = cam.detach().numpy()\n",
    "        #plot_jet_colormap(cam_numpy)\n",
    "        signal_numpy = signal.detach().numpy().squeeze(0)\n",
    "        plot_ecg_high(signal_numpy.T, title = 'ST/T Change', data_1000=cam_numpy)\n",
    "        counter += 1\n",
    "# Define classes\n",
    "classes = ('Norm', 'ST/T Change')\n",
    "title = f'Confusion Matrix - Norm and ST/T Change\\n'\n",
    "plot_confusion_matrix(y_true, y_pred, classes, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal vs Conduction Disturbance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick Conduction Disturbance and normal\n",
    "labels = Z_test.to_numpy()\n",
    "## Check if the first and fourth positions match [1, 0, 0, 0, 0] or [0, 0, 0, 1, 0]\n",
    "# conditions = ((labels == [1, 0, 0, 0, 0]).all(axis=1)) | ((labels == [0, 0, 0, 1, 0]).all(axis=1))\n",
    "conditions = (\n",
    "    ((labels[:, 0] == 1) & (labels[:, 1:] == 0).all(axis=1)) |  \n",
    "    (labels[:, 3] == 1) \n",
    ")\n",
    "indices = np.where(conditions)[0]\n",
    "Z_test_ = labels[indices]\n",
    "Y_test_ = Y_test_scaled[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PTBLoader(Y_test_, Z_test_)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "# evaluate\n",
    "y_pred = []\n",
    "y_true = []\n",
    "threshold = 0.5\n",
    "counter = 0\n",
    "for idx, data in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "    signal, label = data\n",
    "    output, cam = model(signal) # Feed Network\n",
    "    output = output.squeeze()\n",
    "    op_prob = torch.detach(output).numpy().item()\n",
    "    # Convert probabilities to predicted class labels (0 or 1)\n",
    "    op_label = 1 if op_prob > threshold else 0\n",
    "    y_pred.append(op_label)\n",
    "    label_ = label.data.cpu().numpy().item()\n",
    "    y_true.append(label_) # Save Truth\n",
    "    # lets try to view few abnormal ecgs with XAI\n",
    "    if op_label == 1 and label_ == 1 and counter < sample_to_visualize:\n",
    "        cam_numpy = cam.detach().numpy()\n",
    "        #plot_jet_colormap(cam_numpy)\n",
    "        signal_numpy = signal.detach().numpy().squeeze(0)\n",
    "        plot_ecg_high(signal_numpy.T, title = 'Conduction Disturbance', data_1000=cam_numpy)\n",
    "        counter += 1\n",
    "# Define classes\n",
    "classes = ('Norm', 'Conduction\\n Disturbance')\n",
    "title = f'Confusion Matrix - Norm and Conduction Disturbance\\n'\n",
    "plot_confusion_matrix(y_true, y_pred, classes, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal vs Hypertrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick Hypertrophy and normal\n",
    "labels = Z_test.to_numpy()\n",
    "#conditions = ((labels == [1, 0, 0, 0, 0]).all(axis=1)) | ((labels == [0, 0, 0, 0, 1]).all(axis=1))\n",
    "conditions = (\n",
    "    ((labels[:, 0] == 1) & (labels[:, 1:] == 0).all(axis=1)) |  \n",
    "    (labels[:, 4] == 1) \n",
    ")\n",
    "indices = np.where(conditions)[0]\n",
    "Z_test_ = labels[indices]\n",
    "Y_test_ = Y_test_scaled[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PTBLoader(Y_test_, Z_test_)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "# evaluate\n",
    "y_pred = []\n",
    "y_true = []\n",
    "threshold = 0.5\n",
    "counter = 0\n",
    "for idx, data in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "    signal, label = data\n",
    "    output, cam = model(signal) # Feed Network\n",
    "    output = output.squeeze()\n",
    "    op_prob = torch.detach(output).numpy().item()\n",
    "    # Convert probabilities to predicted class labels (0 or 1)\n",
    "    op_label = 1 if op_prob > threshold else 0\n",
    "    y_pred.append(op_label)\n",
    "    label_ = label.data.cpu().numpy().item()\n",
    "    y_true.append(label_) # Save Truth\n",
    "    # lets try to view few abnormal ecgs with XAI\n",
    "    if op_label == 1 and label_ == 1 and counter < sample_to_visualize:\n",
    "        cam_numpy = cam.detach().numpy()\n",
    "        #plot_jet_colormap(cam_numpy)\n",
    "        signal_numpy = signal.detach().numpy().squeeze(0)\n",
    "        plot_ecg_high(signal_numpy.T, title = 'Hypertrophy', data_1000=cam_numpy)\n",
    "        counter += 1\n",
    "# Define classes\n",
    "classes = ('Norm', 'Hypertrophy')\n",
    "title = f'Confusion Matrix - Norm and Hypertrophy\\n'\n",
    "plot_confusion_matrix(y_true, y_pred, classes, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Create a modified 'jet' colormap with alpha=0.4\n",
    "jet = plt.cm.get_cmap('jet', 256)\n",
    "jet_colors = jet(np.linspace(0, 1, 256))\n",
    "jet_colors[:, -1] = 0.4  # Set alpha to 0.4\n",
    "jet_alpha = ListedColormap(jet_colors)\n",
    "\n",
    "# Create a figure\n",
    "fig, ax = plt.subplots(figsize=(12, 3))  # Adjust height for better proportions\n",
    "\n",
    "# Create a dummy image to generate the colorbar\n",
    "dummy_data = np.linspace(0, 1, 100).reshape(1, -1)\n",
    "img = ax.imshow(dummy_data, cmap=jet_alpha, aspect=\"auto\", visible=False)\n",
    "\n",
    "# Create the colorbar\n",
    "cbar = fig.colorbar(img, ax=ax, orientation=\"horizontal\", aspect=30, pad=0.2)\n",
    "cbar.ax.xaxis.set_ticks_position('top')  # Move ticks to the top\n",
    "cbar.ax.tick_params(labelsize=14)  # Increase tick size\n",
    "\n",
    "# Set custom tick labels at 0, 0.5, and 1\n",
    "cbar.set_ticks([0, 0.5, 1])\n",
    "cbar.set_ticklabels([\"0\", \"0.5\", \"1\"])\n",
    "\n",
    "# Manually add \"Intensity\" label to the right\n",
    "cbar.ax.text(1.02, 0.5, \"Intensity\", fontsize=16, fontweight=\"bold\",\n",
    "             transform=cbar.ax.transAxes, va=\"center\")\n",
    "\n",
    "# Remove the main axis\n",
    "ax.remove()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model FLOPS comparisson (both models in evaluation mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "input_data = torch.randn(1, 12, 1000)\n",
    "ecg_leads_count = 12\n",
    "classification_model = SignalCNN(ecg_leads_count, conv_filters=16, number_of_classes=2, add_cam=False)\n",
    "classification_model.eval()\n",
    "\n",
    "# Get parameter count and FLOPs for model without CAM\n",
    "classification_model_params = sum(p.numel() for p in classification_model.parameters())\n",
    "print(f'Classification model parameters: {classification_model_params}')\n",
    "\n",
    "classification_flops = FlopCountAnalysis(classification_model, input_data)\n",
    "classification_model_flops = classification_flops.total()\n",
    "print(f'Classification model FLOPs: {classification_model_flops}')\n",
    "\n",
    "# Model with CAM\n",
    "classification_cam_model = SignalCNN(ecg_leads_count, conv_filters=16, number_of_classes=2, add_cam=True)\n",
    "classification_cam_model.eval()\n",
    "\n",
    "classification_cam_model_params = sum(p.numel() for p in classification_cam_model.parameters())\n",
    "print(f'Classification model with CAM parameters: {classification_cam_model_params}')\n",
    "\n",
    "classification_cam_flops = FlopCountAnalysis(classification_cam_model, input_data)\n",
    "classification_cam_model_flops = classification_cam_flops.total()\n",
    "print(f'Classification model with CAM FLOPs: {classification_cam_model_flops}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Data for the bar plot\n",
    "models = ['Classification', 'Classification +\\nCAM Explainability']\n",
    "flops = [classification_model_flops, classification_cam_model_flops]\n",
    "\n",
    "# Create the horizontal bar plot with bars having minimal gap\n",
    "plt.figure(figsize=(6, 2.5), facecolor='white')  # Slightly smaller figure to reduce space\n",
    "plasma = cm.plasma\n",
    "bars = plt.barh(models, flops, color=[plasma(0.1, alpha=0.8), plasma(0.5, alpha=0.8)], \n",
    "                height=0.6, edgecolor='black', linewidth=1)  # Height adjusted for minimal gap\n",
    "plt.gca().set_facecolor('white')  # Ensures the axes background is white\n",
    "\n",
    "# Remove y-axis labels (since we put them inside bars)\n",
    "plt.yticks([])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('FLOPS', fontsize=18)  # Increased font size for xlabel\n",
    "#plt.title('Comparison of Floating-Point\\nOperations per Second (FLOPS)', fontsize=20)  # Increased font size for title\n",
    "plt.xticks(fontsize=14)  # Increased font size for x-axis ticks\n",
    "plt.xlim(0, max(flops) * 1.2)  # Adjust x-limit for better visualization\n",
    "plt.grid(axis='x', linestyle='--', color='gray', linewidth=0.8, alpha=1.0)\n",
    "\n",
    "# Display model names inside the bars\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_width() * 0.05,  # Position text slightly inside the bar from left\n",
    "             bar.get_y() + bar.get_height() / 2,  \n",
    "             models[i],  # Model name only\n",
    "             va='center', ha='left',  \n",
    "             fontsize=17, fontweight='bold', color='white')\n",
    "\n",
    "# Display rotated FLOPS values on top of the bars\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_width() + (max(flops) * 0.02),  # Position slightly to the right of the bar\n",
    "             bar.get_y() + bar.get_height() / 2,  \n",
    "             f\"{flops[i]}\",  # FLOPS value\n",
    "             va='center', ha='left',  \n",
    "             fontsize=19, fontweight='bold', color='black',\n",
    "             rotation=-90)  # Rotate the text for better readability\n",
    "\n",
    "# Remove extra space between the bars\n",
    "plt.subplots_adjust(left=0.2, right=0.85, top=0.85, bottom=0.15)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1136210,
     "sourceId": 1905968,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30260,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
